{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4171e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a4edc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName(\"Processing_Data\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e338b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version =  3.5.0\n"
     ]
    }
   ],
   "source": [
    "print(\"spark version = \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b7e71c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce93f8c",
   "metadata": {},
   "source": [
    "# Explode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7bbea9",
   "metadata": {},
   "source": [
    "### Q). Convert list of values in a single row to multiple rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a21b126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| ID|     Name|\n",
      "+---+---------+\n",
      "|  1|[A, B, C]|\n",
      "|  2|[X, Y, Z]|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GIven Data:\n",
    "df = spark.createDataFrame([(1,[\"A\", \"B\", \"C\"]), (2, [\"X\", \"Y\", \"Z\"])], [\"ID\", \"Name\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76726986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| ID|Names|\n",
      "+---+-----+\n",
      "|  1|    A|\n",
      "|  1|    B|\n",
      "|  1|    C|\n",
      "|  2|    X|\n",
      "|  2|    Y|\n",
      "|  2|    Z|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using explode() function we can convert into multiple rows\n",
    "df.select(\"ID\", explode(\"Name\").alias(\"Names\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2490e0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9444f6d6",
   "metadata": {},
   "source": [
    "### Q). Word count program by reading txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ba837417",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|value                       |\n",
      "+----------------------------+\n",
      "|Hello world hello computer  |\n",
      "|Laptop mouse keyboard laptop|\n",
      "|hello monitor               |\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Given Data - same as below cell\n",
    "\n",
    "#Read using Dataframe API , truncate=False means it will show full content of each row.\n",
    "df = spark.read.text(\"Input_Files_Pyspark/pyspark_textfile.txt\").show(truncate=False)\n",
    "\n",
    "#Herre by default \"value\" column will come if you read textfile using dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a042f078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello world hello computer ',\n",
       " 'Laptop mouse keyboard laptop',\n",
       " 'hello monitor']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Given Data - same as above cell\n",
    "\n",
    "#Read using RDD API - same as above\n",
    "rdd = spark.sparkContext.textFile(\"Input_Files_Pyspark/pyspark_textfile.txt\")\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c3388ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fd448071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use flatMap(), map(), reduceByKey(), split(), lambda functions\n",
    "\n",
    "rdd2 = rdd.flatMap(lambda line: line.split(\" \"))\n",
    "rdd3 = rdd2.map(lambda word: (word.lower(), 1))\n",
    "rdd4 = rdd3.reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "#or combined all above 3 steps in single line: So rdd4, rdd5 are same.\n",
    "rdd5 = rdd.flatMap(lambda x: x.split()).map(lambda x:(x.lower(), 1)).reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "24d70459",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> rdd2.collect(): \n",
      " ['Hello', 'world', 'hello', 'computer', '', 'Laptop', 'mouse', 'keyboard', 'laptop', 'hello', 'monitor']\n",
      "\n",
      "\n",
      " -> rdd3.collect(): \n",
      " [('hello', 1), ('world', 1), ('hello', 1), ('computer', 1), ('', 1), ('laptop', 1), ('mouse', 1), ('keyboard', 1), ('laptop', 1), ('hello', 1), ('monitor', 1)]\n",
      "\n",
      "\n",
      " -> rdd4.collect(): (note: rdd4, rdd5 are same) \n",
      " [('hello', 3), ('world', 1), ('', 1), ('laptop', 2), ('computer', 1), ('mouse', 1), ('keyboard', 1), ('monitor', 1)]\n",
      "\n",
      "\n",
      " -> rdd5.collect(): (note: rdd4, rdd5 are same) \n",
      " [('hello', 3), ('world', 1), ('laptop', 2), ('computer', 1), ('mouse', 1), ('keyboard', 1), ('monitor', 1)]\n"
     ]
    }
   ],
   "source": [
    "#check above each rdd's data: rdd2, rdd3, rdd4\n",
    "\n",
    "print(\"\\n -> rdd2.collect(): \\n\", rdd2.collect())\n",
    "print(\"\\n\\n -> rdd3.collect(): \\n\", rdd3.collect())\n",
    "print(\"\\n\\n -> rdd4.collect(): (note: rdd4, rdd5 are same) \\n\", rdd4.collect())\n",
    "print(\"\\n\\n -> rdd5.collect(): (note: rdd4, rdd5 are same) \\n\", rdd5.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a185da19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    Word|Count|\n",
      "+--------+-----+\n",
      "|   hello|    3|\n",
      "|  laptop|    2|\n",
      "|computer|    1|\n",
      "|   world|    1|\n",
      "|   mouse|    1|\n",
      "|        |    1|\n",
      "|keyboard|    1|\n",
      "| monitor|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convert above rdd into some Dataframe type\n",
    "\n",
    "df = spark.createDataFrame(rdd4, schema = [\"Word\", \"Count\"])\n",
    "df.orderBy(\"Count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8686a631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fadc2dac",
   "metadata": {},
   "source": [
    "### Q). Difference between flatMap(), map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9cf56476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello world hello computer ',\n",
       " 'Laptop mouse keyboard laptop',\n",
       " 'hello monitor']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Given data:\n",
    "\n",
    "rdd = spark.sparkContext.textFile(\"Input_Files_Pyspark/pyspark_textfile.txt\")\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "131cc74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'world', 'hello', 'computer', ''],\n",
       " ['Laptop', 'mouse', 'keyboard', 'laptop'],\n",
       " ['hello', 'monitor']]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use map(): mapping is not flatten\n",
    "\n",
    "rdd_map = rdd.map(lambda x: x.split(\" \"))\n",
    "rdd_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "66e31b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'world',\n",
       " 'hello',\n",
       " 'computer',\n",
       " '',\n",
       " 'Laptop',\n",
       " 'mouse',\n",
       " 'keyboard',\n",
       " 'laptop',\n",
       " 'hello',\n",
       " 'monitor']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use flatMap() to flatten the nested lists:\n",
    "\n",
    "rdd_flatMap = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "rdd_flatMap.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a390b4",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dad95ad",
   "metadata": {},
   "source": [
    "# Pyspark Regular Expression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6eb9c1",
   "metadata": {},
   "source": [
    "### Q). Identify the valid phone numbers from column using rlike() or regexp_like()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9390adf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.printSchema of DataFrame[Name: string, PhoneNumber: string]>\n",
      "+----+-----------+\n",
      "|Name|PhoneNumber|\n",
      "+----+-----------+\n",
      "|ABCD|      12345|\n",
      "|EFGH|      U6789|\n",
      "|PQRS|      34567|\n",
      "|MNOP|      6789B|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Given data\n",
    "\n",
    "df = spark.createDataFrame([(\"ABCD\", \"12345\"), (\"EFGH\", \"U6789\"), (\"PQRS\", \"34567\"), (\"MNOP\", \"6789B\")], \n",
    "                           schema=[\"Name\", \"PhoneNumber\"])\n",
    "print(df.printSchema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d638a26f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|Name|PhoneNumber|\n",
      "+----+-----------+\n",
      "|ABCD|      12345|\n",
      "|PQRS|      34567|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using rlike() or regexp_like() function with regular expression\n",
    "\n",
    "df.select(\"*\").filter(col(\"PhoneNumber\").rlike(\"^[0-9]*$\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2adf56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a07010ba",
   "metadata": {},
   "source": [
    "### Q). translate() function for character level substitutions in strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "167990ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|    ColumnName|\n",
      "+--------------+\n",
      "|abc123aa_bb_cc|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Given data:\n",
    "\n",
    "df = spark.createDataFrame([(\"abc123aa_bb_cc\",)], [\"ColumnName\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "29a0f003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|TranslatedColumn|\n",
      "+----------------+\n",
      "|  xyz123xx_yy_zz|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now replace \"abc\" with \"xyz\" such that a->x , b->y , c->z can be replaced in single statement: (character level)\n",
    "\n",
    "df.select(translate(\"ColumnName\", \"abc\", \"xyz\").alias(\"TranslatedColumn\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04559e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a74faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354f7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b6da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aba634d1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927ee18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21394033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccf36d04",
   "metadata": {},
   "source": [
    "### Z1).  Lets say in orders table orderID, date, cost, profit columns there. in profit column positive and negative values will be there. you need to find maximum length of continuous negative profits and starting index on this negative series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "318f44e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define proper schema, so that at the time of filters and joins no issues will occur.\n",
    "\n",
    "schema=StructType([StructField(\"orderID\", IntegerType(), True),\\\n",
    "                  StructField(\"orderDate\", DateType(),True),\\\n",
    "                  StructField(\"cost\", IntegerType(), True),\\\n",
    "                  StructField(\"profit\", DoubleType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bcbb46c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+------+\n",
      "|orderID| orderDate|cost|profit|\n",
      "+-------+----------+----+------+\n",
      "|      1|2022-01-01| 100|  10.5|\n",
      "|      2|2022-01-02| 150| -5.25|\n",
      "|      3|2022-01-03| 120| -8.75|\n",
      "|      4|2022-01-04| 200| -15.5|\n",
      "|      5|2022-01-05| 180| 12.75|\n",
      "|      6|2022-01-06|  90|  3.25|\n",
      "|      7|2022-01-07| 110| -20.0|\n",
      "|      8|2022-01-08| 130|  -5.5|\n",
      "|      9|2022-01-09| 180| 12.75|\n",
      "|     10|2022-01-10|  90|  4.25|\n",
      "|     11|2022-01-11| 110| -20.0|\n",
      "|     12|2022-01-12| 130|  -5.5|\n",
      "|     13|2022-01-13| 110| -21.0|\n",
      "|     14|2022-01-14| 130|  -5.5|\n",
      "+-------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the sample data by reading file and convert dataframe to tempview for sql queries.\n",
    "\n",
    "df = spark.read.csv(\"Input_Files_Pyspark/Z_Max_Negative_Sequence.csv\", header=True, schema=schema)\n",
    "df.createOrReplaceTempView(\"Table_1\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85f5a4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- orderID: integer (nullable = true)\n",
      " |-- orderDate: date (nullable = true)\n",
      " |-- cost: integer (nullable = true)\n",
      " |-- profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the schema of dataframe\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5badbc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+------+---+---+-----+\n",
      "|orderID| orderDate|cost|profit| r1| r2|group|\n",
      "+-------+----------+----+------+---+---+-----+\n",
      "|      1|2022-01-01| 100|  10.5|  1|  1|    0|\n",
      "|      5|2022-01-05| 180| 12.75|  5|  2|    3|\n",
      "|      6|2022-01-06|  90|  3.25|  6|  3|    3|\n",
      "|      9|2022-01-09| 180| 12.75|  9|  4|    5|\n",
      "|     10|2022-01-10|  90|  4.25| 10|  5|    5|\n",
      "|      2|2022-01-02| 150| -5.25|  2|  1|    1|\n",
      "|      3|2022-01-03| 120| -8.75|  3|  2|    1|\n",
      "|      4|2022-01-04| 200| -15.5|  4|  3|    1|\n",
      "|      7|2022-01-07| 110| -20.0|  7|  4|    3|\n",
      "|      8|2022-01-08| 130|  -5.5|  8|  5|    3|\n",
      "|     11|2022-01-11| 110| -20.0| 11|  6|    5|\n",
      "|     12|2022-01-12| 130|  -5.5| 12|  7|    5|\n",
      "|     13|2022-01-13| 110| -21.0| 13|  8|    5|\n",
      "|     14|2022-01-14| 130|  -5.5| 14|  9|    5|\n",
      "+-------+----------+----+------+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- first row_number will give row_number based on orderdate, second row_number will be assigned separately for +ve, -ve numbers.\n",
    "- Now all +ve numbers in first partition and all -ve numbers in second partition will be separated. \n",
    "- If we subtract r1-r2 we will get some groups which are all continuous +ve or -ve values. \n",
    "- We can filterout only negative values and then find the largest in count of same group.\n",
    "\"\"\"\n",
    "\n",
    "df2 = spark.sql(\"\"\"\n",
    "                SELECT orderID, orderDate, cost, profit,\n",
    "                ROW_NUMBER() OVER(ORDER BY orderDate) r1,\n",
    "                ROW_NUMBER() OVER(PARTITION BY CASE WHEN profit < 0 then 1 else 0 end ORDER BY orderDate) r2,\n",
    "                ROW_NUMBER() OVER(ORDER BY orderDate) -\n",
    "                ROW_NUMBER() OVER(PARTITION BY CASE WHEN profit < 0 then 1 else 0 end ORDER BY orderDate) as group\n",
    "                FROM Table_1\n",
    "                \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8d5032f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|Start_Index|Max_Negative_seq|\n",
      "+-----------+----------------+\n",
      "|         11|               4|\n",
      "+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- Findout maximum count of same group values and starting index of that max group.\n",
    "\"\"\"\n",
    "\n",
    "df3 = spark.sql(\"\"\"\n",
    "                SELECT MIN(orderID) AS Start_Index, count(*) as Max_Negative_seq\n",
    "                FROM(\n",
    "                SELECT orderID, orderDate, cost, profit,\n",
    "                ROW_NUMBER() OVER(ORDER BY orderDate) r1,\n",
    "                ROW_NUMBER() OVER(PARTITION BY CASE WHEN profit < 0 then 1 else 0 end ORDER BY orderDate) r2,\n",
    "                ROW_NUMBER() OVER(ORDER BY orderDate) -\n",
    "                ROW_NUMBER() OVER(PARTITION BY CASE WHEN profit < 0 then 1 else 0 end ORDER BY orderDate) as group\n",
    "                FROM Table_1) T1 WHERE profit<0\n",
    "                GROUP BY group ORDER BY Max_Negative_seq DESC \n",
    "                LIMIT 1\n",
    "                \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee86c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f73a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb97dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f5f6381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Relation [orderID#470,orderDate#471,cost#472,profit#473] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "orderID: int, orderDate: date, cost: int, profit: double\n",
      "Relation [orderID#470,orderDate#471,cost#472,profit#473] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation [orderID#470,orderDate#471,cost#472,profit#473] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "FileScan csv [orderID#470,orderDate#471,cost#472,profit#473] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/R101100/Desktop/Input_Files_Pyspark/Z_Max_Negative_Sequ..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<orderID:int,orderDate:date,cost:int,profit:double>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Execution plan\n",
    "#Track jobs at -->    http://localhost:4040/jobs/ \n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e811b21",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524cdd20",
   "metadata": {},
   "source": [
    "### Q1). How to read file with multiple delimeters?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
