{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4171e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a4edc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName(\"Processing_Data\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e338b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version =  3.5.0\n"
     ]
    }
   ],
   "source": [
    "print(\"spark version = \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed8fcf",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf36d04",
   "metadata": {},
   "source": [
    "### Z1).  Lets say in orders table orderID, date, cost, profit columns there. in profit column positive and negative values will be there. you need to find maximum length of continuous negative profits and starting index on this negative series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "318f44e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define proper schema, so that at the time of filters and joins no issues will occur.\n",
    "\n",
    "schema=StructType([StructField(\"orderID\", IntegerType(), True),\\\n",
    "                  StructField(\"orderDate\", DateType(),True),\\\n",
    "                  StructField(\"cost\", IntegerType(), True),\\\n",
    "                  StructField(\"profit\", DoubleType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bcbb46c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+------+\n",
      "|orderID| orderDate|cost|profit|\n",
      "+-------+----------+----+------+\n",
      "|      1|2022-01-01| 100|  10.5|\n",
      "|      2|2022-01-02| 150| -5.25|\n",
      "|      3|2022-01-03| 120| -8.75|\n",
      "|      4|2022-01-04| 200| -15.5|\n",
      "|      5|2022-01-05| 180| 12.75|\n",
      "|      6|2022-01-06|  90|  3.25|\n",
      "|      7|2022-01-07| 110| -20.0|\n",
      "|      8|2022-01-08| 130|  -5.5|\n",
      "|      9|2022-01-09| 180| 12.75|\n",
      "|     10|2022-01-10|  90|  4.25|\n",
      "|     11|2022-01-11| 110| -20.0|\n",
      "|     12|2022-01-12| 130|  -5.5|\n",
      "|     13|2022-01-13| 110| -21.0|\n",
      "|     14|2022-01-14| 130|  -5.5|\n",
      "+-------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the sample data by reading file and convert dataframe to tempview for sql queries.\n",
    "\n",
    "df = spark.read.csv(\"Input_Files_Pyspark/Z_Max_Negative_Sequence.csv\", header=True, schema=schema)\n",
    "df.createOrReplaceTempView(\"Table_1\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85f5a4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- orderID: integer (nullable = true)\n",
      " |-- orderDate: date (nullable = true)\n",
      " |-- cost: integer (nullable = true)\n",
      " |-- profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the schema of dataframe\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5badbc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+------+---+---+-----+\n",
      "|orderID| orderDate|cost|profit| r1| r2|group|\n",
      "+-------+----------+----+------+---+---+-----+\n",
      "|      1|2022-01-01| 100|  10.5|  1|  1|    0|\n",
      "|      5|2022-01-05| 180| 12.75|  5|  2|    3|\n",
      "|      6|2022-01-06|  90|  3.25|  6|  3|    3|\n",
      "|      9|2022-01-09| 180| 12.75|  9|  4|    5|\n",
      "|     10|2022-01-10|  90|  4.25| 10|  5|    5|\n",
      "|      2|2022-01-02| 150| -5.25|  2|  1|    1|\n",
      "|      3|2022-01-03| 120| -8.75|  3|  2|    1|\n",
      "|      4|2022-01-04| 200| -15.5|  4|  3|    1|\n",
      "|      7|2022-01-07| 110| -20.0|  7|  4|    3|\n",
      "|      8|2022-01-08| 130|  -5.5|  8|  5|    3|\n",
      "|     11|2022-01-11| 110| -20.0| 11|  6|    5|\n",
      "|     12|2022-01-12| 130|  -5.5| 12|  7|    5|\n",
      "|     13|2022-01-13| 110| -21.0| 13|  8|    5|\n",
      "|     14|2022-01-14| 130|  -5.5| 14|  9|    5|\n",
      "+-------+----------+----+------+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- first row_number will give row_number based on orderdate, second row_number will be assigned separately for +ve, -ve numbers.\n",
    "- Now all +ve numbers in first partition and all -ve numbers in second partition will be separated. \n",
    "- If we subtract r1-r2 we will get some groups which are all continuous +ve or -ve values. \n",
    "- We can filterout only negative values and then find the largest in count of same group.\n",
    "\"\"\"\n",
    "\n",
    "df2 = spark.sql(\"\"\"\n",
    "                SELECT orderID, orderDate, cost, profit,\n",
    "                ROW_NUMBER() OVER(ORDER BY orderDate) r1,\n",
    "                ROW_NUMBER() OVER(PARTITION BY CASE WHEN profit < 0 then 1 else 0 end ORDER BY orderDate) r2,\n",
    "                ROW_NUMBER() OVER(ORDER BY orderDate) -\n",
    "                ROW_NUMBER() OVER(PARTITION BY CASE WHEN profit < 0 then 1 else 0 end ORDER BY orderDate) as group\n",
    "                FROM Table_1\n",
    "                \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8d5032f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|Start_Index|Max_Negative_seq|\n",
      "+-----------+----------------+\n",
      "|         11|               4|\n",
      "+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- Findout maximum count of same group values and starting index of that max group.\n",
    "\"\"\"\n",
    "\n",
    "df3 = spark.sql(\"\"\"\n",
    "                SELECT MIN(orderID) AS Start_Index, count(*) as Max_Negative_seq\n",
    "                FROM(\n",
    "                SELECT orderID, orderDate, cost, profit,\n",
    "                ROW_NUMBER() OVER(ORDER BY orderDate) r1,\n",
    "                ROW_NUMBER() OVER(PARTITION BY CASE WHEN profit < 0 then 1 else 0 end ORDER BY orderDate) r2,\n",
    "                ROW_NUMBER() OVER(ORDER BY orderDate) -\n",
    "                ROW_NUMBER() OVER(PARTITION BY CASE WHEN profit < 0 then 1 else 0 end ORDER BY orderDate) as group\n",
    "                FROM Table_1) T1 WHERE profit<0\n",
    "                GROUP BY group ORDER BY Max_Negative_seq DESC \n",
    "                LIMIT 1\n",
    "                \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f5f6381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Relation [orderID#470,orderDate#471,cost#472,profit#473] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "orderID: int, orderDate: date, cost: int, profit: double\n",
      "Relation [orderID#470,orderDate#471,cost#472,profit#473] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation [orderID#470,orderDate#471,cost#472,profit#473] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "FileScan csv [orderID#470,orderDate#471,cost#472,profit#473] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/R101100/Desktop/Input_Files_Pyspark/Z_Max_Negative_Sequ..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<orderID:int,orderDate:date,cost:int,profit:double>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Execution plan\n",
    "#Track jobs at -->    http://localhost:4040/jobs/ \n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e811b21",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524cdd20",
   "metadata": {},
   "source": [
    "### Q1). How to read file with multiple delimeters?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
