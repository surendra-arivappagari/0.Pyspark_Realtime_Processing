{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4171e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a4edc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName(\"Processing_Data\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e338b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version =  3.5.0\n"
     ]
    }
   ],
   "source": [
    "print(\"spark version = \", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64ba58ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version =  http://DESKTOP-JLC36MP:4040\n"
     ]
    }
   ],
   "source": [
    "#Get the UI weburl for spark session:\n",
    "\n",
    "print(\"spark version = \", spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9beab7",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06e0d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46a67ce4",
   "metadata": {},
   "source": [
    "# Pivot() in pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878911e3",
   "metadata": {},
   "source": [
    "### Q). Convert normal table to pivot in pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42d14133",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----+\n",
      "|    NAME|SUBJECT|MARKS|\n",
      "+--------+-------+-----+\n",
      "|SURENDRA|    PHY|   90|\n",
      "|SURENDRA|   MATH|   95|\n",
      "|SURENDRA|   CHEM|  100|\n",
      "|  KALYAN|    PHY|   90|\n",
      "|  KALYAN|   MATH|  100|\n",
      "|  KALYAN|   CHEM|   83|\n",
      "|     SAI|    BIO|   90|\n",
      "|     SAI|   MATH|   70|\n",
      "|     SAI|   CHEM|   76|\n",
      "|  GANESH|    PHY|   96|\n",
      "|  GANESH|   MATH|   87|\n",
      "|  GANESH|   CHEM|   79|\n",
      "|  GANESH|    BIO|   82|\n",
      "+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Given dataset:\n",
    "_data = [\n",
    "[\"SURENDRA\", \"PHY\", 90],\n",
    "[\"SURENDRA\", \"MATH\", 95],\n",
    "[\"SURENDRA\", \"CHEM\", 100],\n",
    "[\"KALYAN\", \"PHY\", 90],\n",
    "[\"KALYAN\", \"MATH\", 100],\n",
    "[\"KALYAN\", \"CHEM\", 83],\n",
    "[\"SAI\", \"BIO\", 90],\n",
    "[\"SAI\", \"MATH\", 70],\n",
    "[\"SAI\", \"CHEM\", 76],\n",
    "[\"GANESH\", \"PHY\", 96],\n",
    "[\"GANESH\", \"MATH\", 87],\n",
    "[\"GANESH\", \"CHEM\", 79],\n",
    "[\"GANESH\", \"BIO\", 82]\n",
    "]\n",
    "\n",
    "_cols = [\"NAME\", \"SUBJECT\", \"MARKS\"]\n",
    "\n",
    "df =  spark.createDataFrame(_data, schema = _cols)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a80d1049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+----+----+\n",
      "|    NAME| BIO|CHEM|MATH| PHY|\n",
      "+--------+----+----+----+----+\n",
      "|  KALYAN|NULL|  83| 100|  90|\n",
      "|SURENDRA|NULL| 100|  95|  90|\n",
      "|  GANESH|  82|  79|  87|  96|\n",
      "|     SAI|  90|  76|  70|NULL|\n",
      "+--------+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert above dataset into pivot table \n",
    "\n",
    "df2 = df.groupBy(\"NAME\").pivot(\"SUBJECT\").agg(sum(\"MARKS\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8f212b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8d1899c",
   "metadata": {},
   "source": [
    "# Explode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f623029c",
   "metadata": {},
   "source": [
    "### Q). Convert list of values in a single row to multiple rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8534c37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| ID|     Name|\n",
      "+---+---------+\n",
      "|  1|[A, B, C]|\n",
      "|  2|[X, Y, Z]|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GIven Data:\n",
    "df = spark.createDataFrame([(1,[\"A\", \"B\", \"C\"]), (2, [\"X\", \"Y\", \"Z\"])], [\"ID\", \"Name\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cc8031a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| ID|Names|\n",
      "+---+-----+\n",
      "|  1|    A|\n",
      "|  1|    B|\n",
      "|  1|    C|\n",
      "|  2|    X|\n",
      "|  2|    Y|\n",
      "|  2|    Z|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using explode() function we can convert into multiple rows\n",
    "df.select(\"ID\", explode(\"Name\").alias(\"Names\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55013aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb6d964f",
   "metadata": {},
   "source": [
    "### Q). Word count program by reading txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ecd1120b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|value                       |\n",
      "+----------------------------+\n",
      "|Hello world hello computer  |\n",
      "|Laptop mouse keyboard laptop|\n",
      "|hello monitor               |\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Given Data - same as below cell\n",
    "\n",
    "#Read using Dataframe API , truncate=False means it will show full content of each row.\n",
    "df = spark.read.text(\"Input_Files_Pyspark/pyspark_textfile.txt\").show(truncate=False)\n",
    "\n",
    "#Herre by default \"value\" column will come if you read textfile using dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "759bc326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello world hello computer ',\n",
       " 'Laptop mouse keyboard laptop',\n",
       " 'hello monitor']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Given Data - same as above cell\n",
    "\n",
    "#Read using RDD API - same as above\n",
    "rdd = spark.sparkContext.textFile(\"Input_Files_Pyspark/pyspark_textfile.txt\")\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1a24859d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "774c8563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use flatMap(), map(), reduceByKey(), split(), lambda functions\n",
    "\n",
    "rdd2 = rdd.flatMap(lambda line: line.split(\" \"))\n",
    "rdd3 = rdd2.map(lambda word: (word.lower(), 1))\n",
    "rdd4 = rdd3.reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "#or combined all above 3 steps in single line: So rdd4, rdd5 are same.\n",
    "rdd5 = rdd.flatMap(lambda x: x.split()).map(lambda x:(x.lower(), 1)).reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6ee446d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> rdd2.collect(): \n",
      " ['Hello', 'world', 'hello', 'computer', '', 'Laptop', 'mouse', 'keyboard', 'laptop', 'hello', 'monitor']\n",
      "\n",
      "\n",
      " -> rdd3.collect(): \n",
      " [('hello', 1), ('world', 1), ('hello', 1), ('computer', 1), ('', 1), ('laptop', 1), ('mouse', 1), ('keyboard', 1), ('laptop', 1), ('hello', 1), ('monitor', 1)]\n",
      "\n",
      "\n",
      " -> rdd4.collect(): (note: rdd4, rdd5 are same) \n",
      " [('hello', 3), ('world', 1), ('', 1), ('laptop', 2), ('computer', 1), ('mouse', 1), ('keyboard', 1), ('monitor', 1)]\n",
      "\n",
      "\n",
      " -> rdd5.collect(): (note: rdd4, rdd5 are same) \n",
      " [('hello', 3), ('world', 1), ('laptop', 2), ('computer', 1), ('mouse', 1), ('keyboard', 1), ('monitor', 1)]\n"
     ]
    }
   ],
   "source": [
    "#check above each rdd's data: rdd2, rdd3, rdd4\n",
    "\n",
    "print(\"\\n -> rdd2.collect(): \\n\", rdd2.collect())\n",
    "print(\"\\n\\n -> rdd3.collect(): \\n\", rdd3.collect())\n",
    "print(\"\\n\\n -> rdd4.collect(): (note: rdd4, rdd5 are same) \\n\", rdd4.collect())\n",
    "print(\"\\n\\n -> rdd5.collect(): (note: rdd4, rdd5 are same) \\n\", rdd5.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "af4268e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    Word|Count|\n",
      "+--------+-----+\n",
      "|   hello|    3|\n",
      "|  laptop|    2|\n",
      "|computer|    1|\n",
      "|   world|    1|\n",
      "|   mouse|    1|\n",
      "|        |    1|\n",
      "|keyboard|    1|\n",
      "| monitor|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convert above rdd into some Dataframe type\n",
    "\n",
    "df = spark.createDataFrame(rdd4, schema = [\"Word\", \"Count\"])\n",
    "df.orderBy(\"Count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f642fb90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "156f0854",
   "metadata": {},
   "source": [
    "### Q). Difference between flatMap(), map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a81b620e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello world hello computer ',\n",
       " 'Laptop mouse keyboard laptop',\n",
       " 'hello monitor']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Given data:\n",
    "\n",
    "rdd = spark.sparkContext.textFile(\"Input_Files_Pyspark/pyspark_textfile.txt\")\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fff79bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'world', 'hello', 'computer', ''],\n",
       " ['Laptop', 'mouse', 'keyboard', 'laptop'],\n",
       " ['hello', 'monitor']]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use map(): mapping is not flatten\n",
    "\n",
    "rdd_map = rdd.map(lambda x: x.split(\" \"))\n",
    "rdd_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6e954e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'world',\n",
       " 'hello',\n",
       " 'computer',\n",
       " '',\n",
       " 'Laptop',\n",
       " 'mouse',\n",
       " 'keyboard',\n",
       " 'laptop',\n",
       " 'hello',\n",
       " 'monitor']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use flatMap() to flatten the nested lists:\n",
    "\n",
    "rdd_flatMap = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "rdd_flatMap.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f874291",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b368d",
   "metadata": {},
   "source": [
    "# Pyspark Regular Expression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daed13f",
   "metadata": {},
   "source": [
    "### Q). Identify the valid phone numbers from column using rlike() or regexp_like()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6a28123",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.printSchema of DataFrame[Name: string, PhoneNumber: string]>\n",
      "+----+-----------+\n",
      "|Name|PhoneNumber|\n",
      "+----+-----------+\n",
      "|ABCD|      12345|\n",
      "|EFGH|      U6789|\n",
      "|PQRS|      34567|\n",
      "|MNOP|      6789B|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Given data\n",
    "\n",
    "df = spark.createDataFrame([(\"ABCD\", \"12345\"), (\"EFGH\", \"U6789\"), (\"PQRS\", \"34567\"), (\"MNOP\", \"6789B\")], \n",
    "                           schema=[\"Name\", \"PhoneNumber\"])\n",
    "print(df.printSchema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d05d6c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|Name|PhoneNumber|\n",
      "+----+-----------+\n",
      "|ABCD|      12345|\n",
      "|PQRS|      34567|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using rlike() or regexp_like() function with regular expression\n",
    "\n",
    "df.select(\"*\").filter(col(\"PhoneNumber\").rlike(\"^[0-9]*$\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a218e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "508417de",
   "metadata": {},
   "source": [
    "### Q). translate() function for character level substitutions in strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e647ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|    ColumnName|\n",
      "+--------------+\n",
      "|abc123aa_bb_cc|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Given data:\n",
    "\n",
    "df = spark.createDataFrame([(\"abc123aa_bb_cc\",)], [\"ColumnName\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d3f5541c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|TranslatedColumn|\n",
      "+----------------+\n",
      "|  xyz123xx_yy_zz|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now replace \"abc\" with \"xyz\" such that a->x , b->y , c->z can be replaced in single statement: (character level)\n",
    "\n",
    "df.select(translate(\"ColumnName\", \"abc\", \"xyz\").alias(\"TranslatedColumn\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b3c35c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b61e284",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9602f1e",
   "metadata": {},
   "source": [
    "### Spark Joins: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c2efa3",
   "metadata": {},
   "source": [
    "##### Left join, left semi, left anti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "488609fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#given data: \n",
    "\n",
    "data_employees = [(1, \"John\", 1), (2, \"Emma\", 2), (3, \"Raj\", None), (4, \"Nina\", 4)]\n",
    "data_departments = [(1, \"HR\"), (2, \"Tech\"), (3, \"Marketing\"), (None, \"Temp\")]\n",
    "\n",
    "columns_employees = [\"emp_id\", \"emp_name\", \"dept_id\"]\n",
    "columns_departments = [\"dept_id\", \"dept_name\"]\n",
    "\n",
    "df_emp = spark.createDataFrame(data_employees, columns_employees)\n",
    "df_dept = spark.createDataFrame(data_departments, columns_departments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732e58ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+\n",
      "|emp_id|emp_name|dept_id|\n",
      "+------+--------+-------+\n",
      "|     1|    John|      1|\n",
      "|     2|    Emma|      2|\n",
      "|     3|     Raj|   NULL|\n",
      "|     4|    Nina|      4|\n",
      "+------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bffdd98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|dept_id|dept_name|\n",
      "+-------+---------+\n",
      "|      1|       HR|\n",
      "|      2|     Tech|\n",
      "|      3|Marketing|\n",
      "|   NULL|     Temp|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc05129c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+-------+---------+\n",
      "|emp_id|emp_name|dept_id|dept_id|dept_name|\n",
      "+------+--------+-------+-------+---------+\n",
      "|1     |John    |1      |1      |HR       |\n",
      "|2     |Emma    |2      |2      |Tech     |\n",
      "|3     |Raj     |NULL   |NULL   |NULL     |\n",
      "|4     |Nina    |4      |NULL   |NULL     |\n",
      "+------+--------+-------+-------+---------+\n",
      "\n",
      "+------+--------+-------+-------+---------+\n",
      "|emp_id|emp_name|dept_id|dept_id|dept_name|\n",
      "+------+--------+-------+-------+---------+\n",
      "|1     |John    |1      |1      |HR       |\n",
      "|2     |Emma    |2      |2      |Tech     |\n",
      "|3     |Raj     |NULL   |NULL   |NULL     |\n",
      "|4     |Nina    |4      |NULL   |NULL     |\n",
      "+------+--------+-------+-------+---------+\n",
      "\n",
      "+------+--------+-------+-------+---------+\n",
      "|emp_id|emp_name|dept_id|dept_id|dept_name|\n",
      "+------+--------+-------+-------+---------+\n",
      "|1     |John    |1      |1      |HR       |\n",
      "|2     |Emma    |2      |2      |Tech     |\n",
      "|3     |Raj     |NULL   |NULL   |NULL     |\n",
      "|4     |Nina    |4      |NULL   |NULL     |\n",
      "+------+--------+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#both left, leftouter, left_outer are same:\n",
    "\n",
    "df_emp.join(df_dept,df_emp.dept_id ==  df_dept.dept_id,\"left\").show(truncate=False)\n",
    "df_emp.join(df_dept,df_emp.dept_id ==  df_dept.dept_id,\"leftouter\").show(truncate=False)\n",
    "df_emp.join(df_dept,df_emp.dept_id ==  df_dept.dept_id,\"left_outer\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "309f57b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+\n",
      "|emp_id|emp_name|dept_id|\n",
      "+------+--------+-------+\n",
      "|1     |John    |1      |\n",
      "|2     |Emma    |2      |\n",
      "+------+--------+-------+\n",
      "\n",
      "+------+--------+-------+\n",
      "|emp_id|emp_name|dept_id|\n",
      "+------+--------+-------+\n",
      "|1     |John    |1      |\n",
      "|2     |Emma    |2      |\n",
      "+------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# leftsemi -> gives inner join rows+ left columns\n",
    "\n",
    "df_emp.join(df_dept,df_emp.dept_id ==  df_dept.dept_id,\"leftsemi\").show(truncate=False)\n",
    "df_emp.join(df_dept,df_emp.dept_id ==  df_dept.dept_id,\"left_semi\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "954a309a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+\n",
      "|emp_id|emp_name|dept_id|\n",
      "+------+--------+-------+\n",
      "|3     |Raj     |NULL   |\n",
      "|4     |Nina    |4      |\n",
      "+------+--------+-------+\n",
      "\n",
      "+------+--------+-------+\n",
      "|emp_id|emp_name|dept_id|\n",
      "+------+--------+-------+\n",
      "|3     |Raj     |NULL   |\n",
      "|4     |Nina    |4      |\n",
      "+------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# leftanti -> gives (lefttable rows - innerjoin rows)+ left columns\n",
    "\n",
    "df_emp.join(df_dept,df_emp.dept_id ==  df_dept.dept_id,\"leftanti\").show(truncate=False)\n",
    "df_emp.join(df_dept,df_emp.dept_id ==  df_dept.dept_id,\"left_anti\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a108f5",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec9110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897bfbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccf36d04",
   "metadata": {},
   "source": [
    "### Z1).  Lets say in orders table orderID, date, cost, profit columns there. in profit column positive and negative values will be there. you need to find maximum length of continuous negative profits and starting index on this negative series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "318f44e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define proper schema, so that at the time of filters and joins no issues will occur.\n",
    "\n",
    "schema=StructType([StructField(\"orderID\", IntegerType(), True),\\\n",
    "                  StructField(\"orderDate\", DateType(),True),\\\n",
    "                  StructField(\"cost\", IntegerType(), True),\\\n",
    "                  StructField(\"profit\", DoubleType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bcbb46c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+------+\n",
      "|orderID| orderDate|cost|profit|\n",
      "+-------+----------+----+------+\n",
      "|      1|2022-01-01| 100|  10.5|\n",
      "|      2|2022-01-02| 150| -5.25|\n",
      "|      3|2022-01-03| 120| -8.75|\n",
      "|      4|2022-01-04| 200| -15.5|\n",
      "|      5|2022-01-05| 180| 12.75|\n",
      "|      6|2022-01-06|  90|  3.25|\n",
      "|      7|2022-01-07| 110| -20.0|\n",
      "|      8|2022-01-08| 130|  -5.5|\n",
      "|      9|2022-01-09| 180| 12.75|\n",
      "|     10|2022-01-10|  90|  4.25|\n",
      "|     11|2022-01-11| 110| -20.0|\n",
      "|     12|2022-01-12| 130|  -5.5|\n",
      "|     13|2022-01-13| 110| -21.0|\n",
      "|     14|2022-01-14| 130|  -5.5|\n",
      "+-------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the sample data by reading file and convert dataframe to tempview for sql queries.\n",
    "\n",
    "df = spark.read.csv(\"Input_Files_Pyspark/Z_Max_Negative_Sequence.csv\", header=True, schema=schema)\n",
    "df.createOrReplaceTempView(\"Table_1\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85f5a4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- orderID: integer (nullable = true)\n",
      " |-- orderDate: date (nullable = true)\n",
      " |-- cost: integer (nullable = true)\n",
      " |-- profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the schema of dataframe\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5badbc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+------+---+---+-----+\n",
      "|orderID| orderDate|cost|profit| r1| r2|group|\n",
      "+-------+----------+----+------+---+---+-----+\n",
      "|      1|2022-01-01| 100|  10.5|  1|  1|    0|\n",
      "|      5|2022-01-05| 180| 12.75|  5|  2|    3|\n",
      "|      6|2022-01-06|  90|  3.25|  6|  3|    3|\n",
      "|      9|2022-01-09| 180| 12.75|  9|  4|    5|\n",
      "|     10|2022-01-10|  90|  4.25| 10|  5|    5|\n",
      "|      2|2022-01-02| 150| -5.25|  2|  1|    1|\n",
      "|      3|2022-01-03| 120| -8.75|  3|  2|    1|\n",
      "|      4|2022-01-04| 200| -15.5|  4|  3|    1|\n",
      "|      7|2022-01-07| 110| -20.0|  7|  4|    3|\n",
      "|      8|2022-01-08| 130|  -5.5|  8|  5|    3|\n",
      "|     11|2022-01-11| 110| -20.0| 11|  6|    5|\n",
      "|     12|2022-01-12| 130|  -5.5| 12|  7|    5|\n",
      "|     13|2022-01-13| 110| -21.0| 13|  8|    5|\n",
      "|     14|2022-01-14| 130|  -5.5| 14|  9|    5|\n",
      "+-------+----------+----+------+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- first row_number will give row_number based on orderdate, second row_number will be assigned separately for +ve, -ve numbers.\n",
    "- Now all +ve numbers in first partition and all -ve numbers in second partition will be separated. \n",
    "- If we subtract r1-r2 we will get some groups which are all continuous +ve or -ve values. \n",
    "- We can filterout only negative values and then find the largest in count of same group.\n",
    "\"\"\"\n",
    "\n",
    "df2 = spark.sql(\"\"\"\n",
    "                SELECT orderID, orderDate, cost, profit,\n",
    "                ROW_NUMBER() OVER(ORDER BY orderDate) r1,\n",
    "                ROW_NUMBER() OVER(PARTITION BY CASE WHEN profit < 0 then 1 else 0 end ORDER BY orderDate) r2,\n",
    "                ROW_NUMBER() OVER(ORDER BY orderDate) -\n",
    "                ROW_NUMBER() OVER(PARTITION BY CASE WHEN profit < 0 then 1 else 0 end ORDER BY orderDate) as group\n",
    "                FROM Table_1\n",
    "                \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8d5032f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|Start_Index|Max_Negative_seq|\n",
      "+-----------+----------------+\n",
      "|         11|               4|\n",
      "+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- Findout maximum count of same group values and starting index of that max group.\n",
    "\"\"\"\n",
    "\n",
    "df3 = spark.sql(\"\"\"\n",
    "                SELECT MIN(orderID) AS Start_Index, count(*) as Max_Negative_seq\n",
    "                FROM(\n",
    "                SELECT orderID, orderDate, cost, profit,\n",
    "                ROW_NUMBER() OVER(ORDER BY orderDate) r1,\n",
    "                ROW_NUMBER() OVER(PARTITION BY CASE WHEN profit < 0 then 1 else 0 end ORDER BY orderDate) r2,\n",
    "                ROW_NUMBER() OVER(ORDER BY orderDate) -\n",
    "                ROW_NUMBER() OVER(PARTITION BY CASE WHEN profit < 0 then 1 else 0 end ORDER BY orderDate) as group\n",
    "                FROM Table_1) T1 WHERE profit<0\n",
    "                GROUP BY group ORDER BY Max_Negative_seq DESC \n",
    "                LIMIT 1\n",
    "                \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e3d954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743fbced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669f4a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f5f6381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Relation [orderID#470,orderDate#471,cost#472,profit#473] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "orderID: int, orderDate: date, cost: int, profit: double\n",
      "Relation [orderID#470,orderDate#471,cost#472,profit#473] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation [orderID#470,orderDate#471,cost#472,profit#473] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "FileScan csv [orderID#470,orderDate#471,cost#472,profit#473] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/R101100/Desktop/Input_Files_Pyspark/Z_Max_Negative_Sequ..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<orderID:int,orderDate:date,cost:int,profit:double>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Execution plan\n",
    "#Track jobs at -->    http://localhost:4040/jobs/ \n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e811b21",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524cdd20",
   "metadata": {},
   "source": [
    "### Q1). How to read file with multiple delimeters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fd5b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7460029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f74163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514e184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40318106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95783dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([(1, 2), (3, 4), (3, 6)])\n",
    "reduced_rdd = rdd.reduceByKey(lambda a, b: a + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ebadebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 10)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_rdd.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
