{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Pyspark_VS_Pandas\").getOrCreate()\n",
    "conf = spark.sparkContext._conf.setAll([('spark.driver.memory', '2g'), ('spark.executor.memory', '2g'), ('spark.executor.num','3'), ('spark.network.timeout', '1000000')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1). dataframes with default schama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date         object\n",
      "City         object\n",
      "Temper        int64\n",
      "Wind          int64\n",
      "Humidity    float64\n",
      "dtype: object\n",
      "\n",
      "\n",
      "[('Date', 'string'), ('City', 'string'), ('Temper', 'int'), ('Wind', 'int'), ('Humidity', 'double')]\n",
      "\n",
      "\n",
      "         Date      City  Temper  Wind  Humidity\n",
      "0  01-01-2019  Banglore      35    15      10.5\n",
      "1  01-02-2019  Banglore      34    14       9.9\n",
      "2  01-03-2019  Banglore      33    13       7.9\n",
      "3  01-04-2019  Banglore      36    16       4.5\n",
      "4  01-05-2019       HYD      45     8       9.0\n",
      "\n",
      "\n",
      "+----------+--------+------+----+--------+\n",
      "|      Date|    City|Temper|Wind|Humidity|\n",
      "+----------+--------+------+----+--------+\n",
      "|01-01-2019|Banglore|    35|  15|    10.5|\n",
      "|01-02-2019|Banglore|    34|  14|     9.9|\n",
      "|01-03-2019|Banglore|    33|  13|     7.9|\n",
      "|01-04-2019|Banglore|    36|  16|     4.5|\n",
      "|01-05-2019|     HYD|    45|   8|     9.0|\n",
      "|01-06-2019|     HYD|    47|   7|     7.1|\n",
      "|01-07-2019|     HYD|    49|   8|     8.4|\n",
      "|01-08-2019| Chennai|    50|   2|     7.8|\n",
      "|01-09-2019| Chennai|    52|   3|     9.4|\n",
      "|01-10-2019| Chennai|    54|   1|     7.6|\n",
      "+----------+--------+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1). read csv into pandas pyspark dataframes with default schama\n",
    "dfpd = pd.read_csv(\"Pandas_vs_Pyspark.csv\")\n",
    "dfps = spark.read.load(\"Pandas_vs_Pyspark.csv\", header='true', inferSchema='true', format='com.databricks.spark.csv' )\n",
    "\n",
    "#O/P:\n",
    "print(dfpd.dtypes)\n",
    "print(\"\\n\")\n",
    "print(dfps.dtypes)\n",
    "print(\"\\n\")\n",
    "print(dfpd.head())\n",
    "print(\"\\n\")\n",
    "dfps.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2). dataframes with defined schama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date        datetime64[ns]\n",
      "City                object\n",
      "Temper               int64\n",
      "Wind                 int64\n",
      "Humidity           float64\n",
      "dtype: object\n",
      "\n",
      "\n",
      "[('Date', 'date'), ('City', 'string'), ('Temper', 'int'), ('Wind', 'int'), ('Humidity', 'double')]\n"
     ]
    }
   ],
   "source": [
    "#2). read csv into pandas pyspark dataframes with defined schama\n",
    "dfpd = pd.read_csv(\"Pandas_vs_Pyspark.csv\", parse_dates=[\"Date\"])\n",
    "# dfpd = dfpd.astype(str) #1--> convert all columns to object datatype\n",
    "\n",
    "# col_schema = {\"Date\":'datetime64', \"City\":'str', \"Temper\":'int64', \"Wind\":'int64', \"Humidity\":'float64'}\n",
    "# dfpd = dfpd.astype(col_schema) #2--> convert all columns key value paired\n",
    "\n",
    "dfpd = dfpd.infer_objects() #3). atomatically detect all datatypes \n",
    "print(dfpd.dtypes)\n",
    "########################################\n",
    "\n",
    "# dfps = spark.read.load(\"Pandas_vs_Pyspark.csv\", header='true', inferSchema='true', format='com.databricks.spark.csv' )\n",
    "schema = StructType([\\\n",
    "        StructField(\"Date\", DateType()),\\\n",
    "        StructField(\"City\", StringType()), \\\n",
    "        StructField(\"Temper\", IntegerType()),\\\n",
    "        StructField(\"Wind\", IntegerType()),\\\n",
    "        StructField(\"Humidity\", DoubleType())])\n",
    "dfps = spark.read.csv(\"Pandas_vs_Pyspark.csv\", header='true', schema=schema )\n",
    "\n",
    "print(\"\\n\")\n",
    "print(dfps.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3). Schema information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 5 columns):\n",
      "Date        10 non-null datetime64[ns]\n",
      "City        10 non-null object\n",
      "Temper      10 non-null int64\n",
      "Wind        10 non-null int64\n",
      "Humidity    10 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(1), int64(2), object(1)\n",
      "memory usage: 480.0+ bytes\n",
      "None\n",
      "\n",
      "\n",
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Temper: integer (nullable = true)\n",
      " |-- Wind: integer (nullable = true)\n",
      " |-- Humidity: double (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#3). Schema information \n",
    "print(dfpd.info())\n",
    "print(\"\\n\")\n",
    "print(dfps.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4). Count of datafrmae, Shape, head, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Count###\n",
      "Date        10\n",
      "City        10\n",
      "Temper      10\n",
      "Wind        10\n",
      "Humidity    10\n",
      "dtype: int64\n",
      "10\n",
      "###shape###\n",
      "(10, 5)\n",
      "10 5\n",
      "###head, first###\n",
      "        Date      City  Temper  Wind  Humidity\n",
      "0 2019-01-01  Banglore      35    15      10.5\n",
      "1 2019-01-02  Banglore      34    14       9.9\n",
      "2 2019-01-03  Banglore      33    13       7.9\n",
      "3 2019-01-04  Banglore      36    16       4.5\n",
      "4 2019-01-05       HYD      45     8       9.0\n",
      "+----------+--------+------+----+--------+\n",
      "|      Date|    City|Temper|Wind|Humidity|\n",
      "+----------+--------+------+----+--------+\n",
      "|0006-07-12|Banglore|    35|  15|    10.5|\n",
      "|0006-08-12|Banglore|    34|  14|     9.9|\n",
      "|0006-09-09|Banglore|    33|  13|     7.9|\n",
      "|0006-10-10|Banglore|    36|  16|     4.5|\n",
      "|0006-11-09|     HYD|    45|   8|     9.0|\n",
      "+----------+--------+------+----+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#4). Count of datafrmae\n",
    "print(\"###Count###\")\n",
    "print(dfpd.count())\n",
    "print(dfps.count())\n",
    "\n",
    "print(\"###shape###\")\n",
    "print(dfpd.shape)\n",
    "print(dfps.count(), len(dfps.columns))\n",
    "\n",
    "print(\"###head, first###\")\n",
    "print(dfpd.head(5))\n",
    "print(dfps.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5). Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Temper       Wind   Humidity\n",
      "count  10.000000  10.000000  10.000000\n",
      "mean   43.500000   8.700000   8.210000\n",
      "std     8.154753   5.578729   1.693419\n",
      "min    33.000000   1.000000   4.500000\n",
      "25%    35.250000   4.000000   7.650000\n",
      "50%    46.000000   8.000000   8.150000\n",
      "75%    49.750000  13.750000   9.300000\n",
      "max    54.000000  16.000000  10.500000\n",
      "+-------+--------+-----------------+-----------------+------------------+\n",
      "|summary|    City|           Temper|             Wind|          Humidity|\n",
      "+-------+--------+-----------------+-----------------+------------------+\n",
      "|  count|      10|               10|               10|                10|\n",
      "|   mean|    null|             43.5|              8.7| 8.209999999999999|\n",
      "| stddev|    null|8.154753215150043|5.578729445153459|1.6934186330221677|\n",
      "|    min|Banglore|               33|                1|               4.5|\n",
      "|    max|     HYD|               54|               16|              10.5|\n",
      "+-------+--------+-----------------+-----------------+------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#5). Describe\n",
    "print(dfpd.describe())\n",
    "print(dfps.describe().show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6). Create DataFrame using colletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Name  Weight   Address        DOB  Batch   Salary\n",
      "0    A      70       HYD 1990-01-15   2016  51000.0\n",
      "1    B      61  Banglore 1996-01-19   2017  46500.5\n",
      "2    C      83   Chennai 1999-02-28   2018  52000.0\n",
      "3    D      60    Mumbai 1989-06-13   2016  51000.0\n",
      "4    E      92  Banglore 2000-11-15   2016  52000.0\n",
      "5    F      69    Mumbai 1995-10-12   2017  75000.6\n",
      "6    G      84   Chennai 1998-11-25   2016  64000.5\n",
      "7    H      71  Banglore 1994-09-15   2018  52000.0\n",
      "8    I      77       HYD 1996-01-15   2017  46500.5\n",
      "+----+------+--------+----------+-----+-------+\n",
      "|Name|Weight| Address|       DOB|Batch| Salary|\n",
      "+----+------+--------+----------+-----+-------+\n",
      "|   A|    70|     HYD|1990-01-15| 2016|51000.0|\n",
      "|   B|    61|Banglore|1996-01-19| 2017|46500.5|\n",
      "|   C|    83| Chennai|1999-02-28| 2018|52000.0|\n",
      "|   D|    60|  Mumbai|1989-06-13| 2016|51000.0|\n",
      "|   E|    92|Banglore|2000-11-15| 2016|52000.0|\n",
      "|   F|    69|  Mumbai|1995-10-12| 2017|75000.6|\n",
      "|   G|    84| Chennai|1998-11-25| 2016|64000.5|\n",
      "|   H|    71|Banglore|1994-09-15| 2018|52000.0|\n",
      "|   I|    77|     HYD|1996-01-15| 2017|46500.5|\n",
      "+----+------+--------+----------+-----+-------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#6). Create DataFrame using colletion\n",
    "dict1 = {\"Name\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\"],\\\n",
    "         \"Weight\":[70,61,83,60,92,69,84,71,77],\\\n",
    "         \"Address\":[\"HYD\",\"Banglore\",\"Chennai\",\"Mumbai\",\"Banglore\",\"Mumbai\",\"Chennai\",\"Banglore\",\"HYD\"],\\\n",
    "         \"DOB\":[\"15-01-1990\", \"19-01-1996\", \"28-02-1999\", \"13-06-1989\", \"15-11-2000\", \"10-12-1995\", \"25-11-1998\", \"15-09-1994\", \"15-01-1996\"],\\\n",
    "         \"Batch\":[2016, 2017, 2018, 2016, 2016, 2017, 2016, 2018, 2017],\\\n",
    "         \"Salary\":[51000.00, 46500.50, 52000.00, 51000.00, 52000.00, 75000.60, 64000.50, 52000.00, 46500.50]         \n",
    "        }\n",
    "\n",
    "##\n",
    "dfpd = pd.DataFrame(dict1)\n",
    "dfpd_dtype = {\"Name\":'str', \"Weight\":'int64', \"Address\":'str', \"DOB\":'datetime64', \"Batch\":'int64', \"Salary\":'float64' }\n",
    "dfpd = dfpd.astype(dfpd_dtype)\n",
    "\n",
    "##\n",
    "schema = StructType([\\\n",
    "                     StructField(\"Name\", StringType(), True),\\\n",
    "                     StructField(\"Weight\", IntegerType(), True),\\\n",
    "                     StructField(\"Address\", StringType(), True),\\\n",
    "                     StructField(\"DOB\", DateType(), True),\\\n",
    "                     StructField(\"Batch\", IntegerType(), True),\\\n",
    "                     StructField(\"Salary\", DoubleType(), True)])\n",
    "\n",
    "\n",
    "dfps = spark.createDataFrame(dfpd, schema)\n",
    "print(dfpd.head(10))\n",
    "print(dfps.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7). Add New column to existing datafrmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Name  Weight   Address        DOB  Batch   Salary  Salary_Hike  \\\n",
      "0    A      70       HYD 1990-01-15   2016  51000.0      51000.0   \n",
      "1    B      61  Banglore 1996-01-19   2017  46500.5      46500.5   \n",
      "2    C      83   Chennai 1999-02-28   2018  52000.0      52000.0   \n",
      "3    D      60    Mumbai 1989-06-13   2016  51000.0      51000.0   \n",
      "4    E      92  Banglore 2000-11-15   2016  52000.0      52000.0   \n",
      "\n",
      "   Salary_Hike_1year  Salary_Hike_2year  \n",
      "0            61000.0            71000.0  \n",
      "1            56500.5            66500.5  \n",
      "2            62000.0            72000.0  \n",
      "3            61000.0            71000.0  \n",
      "4            62000.0            72000.0  \n",
      "+----+------+--------+----------+-----+-------+-----------+-----------------+-----------------+\n",
      "|Name|Weight| Address|       DOB|Batch| Salary|Salary_Hike|Salary_Hike_1year|Salary_Hike_2year|\n",
      "+----+------+--------+----------+-----+-------+-----------+-----------------+-----------------+\n",
      "|   A|    70|     HYD|1990-01-15| 2016|51000.0|    51000.0|          61000.0|          71000.0|\n",
      "|   B|    61|Banglore|1996-01-19| 2017|46500.5|    46500.5|          56500.5|          66500.5|\n",
      "|   C|    83| Chennai|1999-02-28| 2018|52000.0|    52000.0|          62000.0|          72000.0|\n",
      "|   D|    60|  Mumbai|1989-06-13| 2016|51000.0|    51000.0|          61000.0|          71000.0|\n",
      "|   E|    92|Banglore|2000-11-15| 2016|52000.0|    52000.0|          62000.0|          72000.0|\n",
      "|   F|    69|  Mumbai|1995-10-12| 2017|75000.6|    75000.6|          85000.6|          95000.6|\n",
      "|   G|    84| Chennai|1998-11-25| 2016|64000.5|    64000.5|          74000.5|          84000.5|\n",
      "|   H|    71|Banglore|1994-09-15| 2018|52000.0|    52000.0|          62000.0|          72000.0|\n",
      "|   I|    77|     HYD|1996-01-15| 2017|46500.5|    46500.5|          56500.5|          66500.5|\n",
      "+----+------+--------+----------+-----+-------+-----------+-----------------+-----------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#7). Add New column to existing datafrmae\n",
    "dfpd[\"Salary_Hike\"] = dfpd[\"Salary\"]\n",
    "dfpd[\"Salary_Hike_1year\"] = dfpd[\"Salary\"]+10000\n",
    "dfpd[\"Salary_Hike_2year\"] = dfpd[\"Salary\"]+20000\n",
    "\n",
    "dfps = dfps.withColumn(\"Salary_Hike\", col(\"Salary\"))\n",
    "dfps = dfps.withColumn(\"Salary_Hike_1year\", col(\"Salary\")+10000)\n",
    "dfps = dfps.withColumn(\"Salary_Hike_2year\", col(\"Salary\")+20000)\n",
    "\n",
    "print(dfpd.head())\n",
    "print(dfps.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8). Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Name  Weight   Address        DOB  Batch   Salary  DeleteThisColumn  \\\n",
      "0    A      70       HYD 1990-01-15   2016  51000.0           51000.0   \n",
      "1    B      61  Banglore 1996-01-19   2017  46500.5           46500.5   \n",
      "2    C      83   Chennai 1999-02-28   2018  52000.0           52000.0   \n",
      "3    D      60    Mumbai 1989-06-13   2016  51000.0           51000.0   \n",
      "4    E      92  Banglore 2000-11-15   2016  52000.0           52000.0   \n",
      "\n",
      "   Salary_Hike_1year  Salary_Hike_2year  \n",
      "0            61000.0            71000.0  \n",
      "1            56500.5            66500.5  \n",
      "2            62000.0            72000.0  \n",
      "3            61000.0            71000.0  \n",
      "4            62000.0            72000.0  \n",
      "+----+------+--------+----------+-----+-------+----------------+-----------------+-----------------+\n",
      "|Name|Weight| Address|       DOB|Batch| Salary|DeleteThisColumn|Salary_Hike_1year|Salary_Hike_2year|\n",
      "+----+------+--------+----------+-----+-------+----------------+-----------------+-----------------+\n",
      "|   A|    70|     HYD|1990-01-15| 2016|51000.0|         51000.0|          61000.0|          71000.0|\n",
      "|   B|    61|Banglore|1996-01-19| 2017|46500.5|         46500.5|          56500.5|          66500.5|\n",
      "|   C|    83| Chennai|1999-02-28| 2018|52000.0|         52000.0|          62000.0|          72000.0|\n",
      "|   D|    60|  Mumbai|1989-06-13| 2016|51000.0|         51000.0|          61000.0|          71000.0|\n",
      "|   E|    92|Banglore|2000-11-15| 2016|52000.0|         52000.0|          62000.0|          72000.0|\n",
      "|   F|    69|  Mumbai|1995-10-12| 2017|75000.6|         75000.6|          85000.6|          95000.6|\n",
      "|   G|    84| Chennai|1998-11-25| 2016|64000.5|         64000.5|          74000.5|          84000.5|\n",
      "|   H|    71|Banglore|1994-09-15| 2018|52000.0|         52000.0|          62000.0|          72000.0|\n",
      "|   I|    77|     HYD|1996-01-15| 2017|46500.5|         46500.5|          56500.5|          66500.5|\n",
      "+----+------+--------+----------+-----+-------+----------------+-----------------+-----------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#8). Rename Columns\n",
    "dfpd = dfpd.rename(columns={\"Salary_Hike\":\"DeleteThisColumn\"})\n",
    "dfps = dfps.withColumnRenamed(\"Salary_Hike\",\"DeleteThisColumn\")\n",
    "print(dfpd.head())\n",
    "print(dfps.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9).Delete Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Name  Weight   Address        DOB  Batch   Salary  Salary_Hike_1year  \\\n",
      "0    A      70       HYD 1990-01-15   2016  51000.0            61000.0   \n",
      "1    B      61  Banglore 1996-01-19   2017  46500.5            56500.5   \n",
      "2    C      83   Chennai 1999-02-28   2018  52000.0            62000.0   \n",
      "3    D      60    Mumbai 1989-06-13   2016  51000.0            61000.0   \n",
      "4    E      92  Banglore 2000-11-15   2016  52000.0            62000.0   \n",
      "\n",
      "   Salary_Hike_2year  \n",
      "0            71000.0  \n",
      "1            66500.5  \n",
      "2            72000.0  \n",
      "3            71000.0  \n",
      "4            72000.0  \n",
      "+----+------+--------+----------+-----+-------+-----------------+-----------------+\n",
      "|Name|Weight| Address|       DOB|Batch| Salary|Salary_Hike_1year|Salary_Hike_2year|\n",
      "+----+------+--------+----------+-----+-------+-----------------+-----------------+\n",
      "|   A|    70|     HYD|1990-01-15| 2016|51000.0|          61000.0|          71000.0|\n",
      "|   B|    61|Banglore|1996-01-19| 2017|46500.5|          56500.5|          66500.5|\n",
      "|   C|    83| Chennai|1999-02-28| 2018|52000.0|          62000.0|          72000.0|\n",
      "|   D|    60|  Mumbai|1989-06-13| 2016|51000.0|          61000.0|          71000.0|\n",
      "|   E|    92|Banglore|2000-11-15| 2016|52000.0|          62000.0|          72000.0|\n",
      "|   F|    69|  Mumbai|1995-10-12| 2017|75000.6|          85000.6|          95000.6|\n",
      "|   G|    84| Chennai|1998-11-25| 2016|64000.5|          74000.5|          84000.5|\n",
      "|   H|    71|Banglore|1994-09-15| 2018|52000.0|          62000.0|          72000.0|\n",
      "|   I|    77|     HYD|1996-01-15| 2017|46500.5|          56500.5|          66500.5|\n",
      "+----+------+--------+----------+-----+-------+-----------------+-----------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#9).Delete Columns\n",
    "dfpd = dfpd.drop(\"DeleteThisColumn\", axis=1)\n",
    "dfps = dfps.drop(\"DeleteThisColumn\")\n",
    "\n",
    "print(dfpd.head())\n",
    "print(dfps.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10). Select Multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Address        DOB  Batch\n",
      "0       HYD 1990-01-15   2016\n",
      "1  Banglore 1996-01-19   2017\n",
      "2   Chennai 1999-02-28   2018\n",
      "3    Mumbai 1989-06-13   2016\n",
      "4  Banglore 2000-11-15   2016\n",
      "+--------+----------+-----+\n",
      "| Address|       DOB|Batch|\n",
      "+--------+----------+-----+\n",
      "|     HYD|1990-01-15| 2016|\n",
      "|Banglore|1996-01-19| 2017|\n",
      "| Chennai|1999-02-28| 2018|\n",
      "|  Mumbai|1989-06-13| 2016|\n",
      "|Banglore|2000-11-15| 2016|\n",
      "+--------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#10). Select Multiple columns\n",
    "print(dfpd[['Address', 'DOB', 'Batch']].head(5))\n",
    "print(dfps.select('Address', 'DOB', 'Batch').show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11). Filter Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Name  Weight   Address        DOB  Batch   Salary  Salary_Hike_1year  \\\n",
      "4    E      92  Banglore 2000-11-15   2016  52000.0            62000.0   \n",
      "7    H      71  Banglore 1994-09-15   2018  52000.0            62000.0   \n",
      "\n",
      "   Salary_Hike_2year  \n",
      "4            72000.0  \n",
      "7            72000.0  \n",
      "+----+------+--------+----------+-----+-------+-----------------+-----------------+\n",
      "|Name|Weight| Address|       DOB|Batch| Salary|Salary_Hike_1year|Salary_Hike_2year|\n",
      "+----+------+--------+----------+-----+-------+-----------------+-----------------+\n",
      "|   E|    92|Banglore|2000-11-15| 2016|52000.0|          62000.0|          72000.0|\n",
      "|   H|    71|Banglore|1994-09-15| 2018|52000.0|          62000.0|          72000.0|\n",
      "+----+------+--------+----------+-----+-------+-----------------+-----------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#11). Filter Conditions\n",
    "dfpd1 = dfpd[(dfpd[\"Address\"]==\"Banglore\") & (dfpd[\"Weight\"]>70)]\n",
    "dfps1 = dfps.filter((col(\"Address\")==\"Banglore\") & (col(\"Weight\")>70))\n",
    "\n",
    "print(dfpd1.head())\n",
    "print(dfps1.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12). Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Name  Weight   Address        DOB  Batch   Salary  Salary_Hike_1year  \\\n",
      "4    E      92  Banglore 2000-11-15   2016  52000.0            62000.0   \n",
      "7    H      71  Banglore 1994-09-15   2018  52000.0            62000.0   \n",
      "1    B      61  Banglore 1996-01-19   2017  46500.5            56500.5   \n",
      "6    G      84   Chennai 1998-11-25   2016  64000.5            74000.5   \n",
      "2    C      83   Chennai 1999-02-28   2018  52000.0            62000.0   \n",
      "0    A      70       HYD 1990-01-15   2016  51000.0            61000.0   \n",
      "8    I      77       HYD 1996-01-15   2017  46500.5            56500.5   \n",
      "5    F      69    Mumbai 1995-10-12   2017  75000.6            85000.6   \n",
      "3    D      60    Mumbai 1989-06-13   2016  51000.0            61000.0   \n",
      "\n",
      "   Salary_Hike_2year  \n",
      "4            72000.0  \n",
      "7            72000.0  \n",
      "1            66500.5  \n",
      "6            84000.5  \n",
      "2            72000.0  \n",
      "0            71000.0  \n",
      "8            66500.5  \n",
      "5            95000.6  \n",
      "3            71000.0  \n",
      "+----+------+--------+----------+-----+-------+-----------------+-----------------+\n",
      "|Name|Weight| Address|       DOB|Batch| Salary|Salary_Hike_1year|Salary_Hike_2year|\n",
      "+----+------+--------+----------+-----+-------+-----------------+-----------------+\n",
      "|   E|    92|Banglore|2000-11-15| 2016|52000.0|          62000.0|          72000.0|\n",
      "|   H|    71|Banglore|1994-09-15| 2018|52000.0|          62000.0|          72000.0|\n",
      "|   B|    61|Banglore|1996-01-19| 2017|46500.5|          56500.5|          66500.5|\n",
      "|   G|    84| Chennai|1998-11-25| 2016|64000.5|          74000.5|          84000.5|\n",
      "|   C|    83| Chennai|1999-02-28| 2018|52000.0|          62000.0|          72000.0|\n",
      "|   A|    70|     HYD|1990-01-15| 2016|51000.0|          61000.0|          71000.0|\n",
      "|   I|    77|     HYD|1996-01-15| 2017|46500.5|          56500.5|          66500.5|\n",
      "|   F|    69|  Mumbai|1995-10-12| 2017|75000.6|          85000.6|          95000.6|\n",
      "|   D|    60|  Mumbai|1989-06-13| 2016|51000.0|          61000.0|          71000.0|\n",
      "+----+------+--------+----------+-----+-------+-----------------+-----------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#12). Sorting\n",
    "dfpd3 = dfpd.sort_values(by=['Address', 'Salary'], ascending=[True, False])\n",
    "\n",
    "dfps3 = dfps.orderBy(\"Address\",\"Salary\", ascending=[True, False])\n",
    "\n",
    "print(dfpd3.head(20))\n",
    "print(dfps3.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13). GroupBy aggregate functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Salary                                          \n",
      "          Count     Total       Average  Minimum  Maximum\n",
      "Address                                                  \n",
      "Banglore    3.0  150500.5  50166.833333  46500.5  52000.0\n",
      "Chennai     2.0  116000.5  58000.250000  52000.0  64000.5\n",
      "HYD         2.0   97500.5  48750.250000  46500.5  51000.0\n",
      "Mumbai      2.0  126000.6  63000.300000  51000.0  75000.6\n",
      "+--------+-----------+\n",
      "| Address|min(Salary)|\n",
      "+--------+-----------+\n",
      "| Chennai|    52000.0|\n",
      "|  Mumbai|    51000.0|\n",
      "|     HYD|    46500.5|\n",
      "|Banglore|    46500.5|\n",
      "+--------+-----------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#13). GroupBy aggregate functions:\n",
    "dfpd4 = dfpd.groupby([\"Address\"]).agg({\"Salary\": [np.size, np.sum, np.mean, np.min, np.max]})\\\n",
    ".rename(columns={\"size\":\"Count\", \"sum\":\"Total\", \"mean\":\"Average\", \"amin\":\"Minimum\", \"amax\":\"Maximum\"})\n",
    "\n",
    "#dfpd4 = dfpd.groupby([\"Address\"])[\"Salary\"].mean()    #--> for specific column aggregate functions\n",
    "#dfpd4 = dfpd.groupby([\"Address\"]).mean()    #--> for all columns aggregate functions\n",
    "\n",
    "dfps4 = dfps.groupBy(col(\"Address\")).agg({\"Salary\":'min'}) #--> 1column\n",
    "\n",
    "# expr = [min(x) for x in dfps.columns]  #--> for all columns\n",
    "# dfps.groupBy(col(\"Address\")).agg(*expr).show()\n",
    "\n",
    "print(dfpd4.head())\n",
    "print(dfps4.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14). Row Number, Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Address</th>\n",
       "      <th>DOB</th>\n",
       "      <th>Batch</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Salary_Hike_1year</th>\n",
       "      <th>Salary_Hike_2year</th>\n",
       "      <th>RN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E</td>\n",
       "      <td>92</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>2000-11-15</td>\n",
       "      <td>2016</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>62000.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>H</td>\n",
       "      <td>71</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>1994-09-15</td>\n",
       "      <td>2018</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>62000.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>61</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>1996-01-19</td>\n",
       "      <td>2017</td>\n",
       "      <td>46500.5</td>\n",
       "      <td>56500.5</td>\n",
       "      <td>66500.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>G</td>\n",
       "      <td>84</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>1998-11-25</td>\n",
       "      <td>2016</td>\n",
       "      <td>64000.5</td>\n",
       "      <td>74000.5</td>\n",
       "      <td>84000.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>83</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>1999-02-28</td>\n",
       "      <td>2018</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>62000.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>70</td>\n",
       "      <td>HYD</td>\n",
       "      <td>1990-01-15</td>\n",
       "      <td>2016</td>\n",
       "      <td>51000.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>71000.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I</td>\n",
       "      <td>77</td>\n",
       "      <td>HYD</td>\n",
       "      <td>1996-01-15</td>\n",
       "      <td>2017</td>\n",
       "      <td>46500.5</td>\n",
       "      <td>56500.5</td>\n",
       "      <td>66500.5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F</td>\n",
       "      <td>69</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>1995-10-12</td>\n",
       "      <td>2017</td>\n",
       "      <td>75000.6</td>\n",
       "      <td>85000.6</td>\n",
       "      <td>95000.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D</td>\n",
       "      <td>60</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>1989-06-13</td>\n",
       "      <td>2016</td>\n",
       "      <td>51000.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>71000.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Name  Weight   Address        DOB  Batch   Salary  Salary_Hike_1year  \\\n",
       "4    E      92  Banglore 2000-11-15   2016  52000.0            62000.0   \n",
       "7    H      71  Banglore 1994-09-15   2018  52000.0            62000.0   \n",
       "1    B      61  Banglore 1996-01-19   2017  46500.5            56500.5   \n",
       "6    G      84   Chennai 1998-11-25   2016  64000.5            74000.5   \n",
       "2    C      83   Chennai 1999-02-28   2018  52000.0            62000.0   \n",
       "0    A      70       HYD 1990-01-15   2016  51000.0            61000.0   \n",
       "8    I      77       HYD 1996-01-15   2017  46500.5            56500.5   \n",
       "5    F      69    Mumbai 1995-10-12   2017  75000.6            85000.6   \n",
       "3    D      60    Mumbai 1989-06-13   2016  51000.0            61000.0   \n",
       "\n",
       "   Salary_Hike_2year   RN  \n",
       "4            72000.0  2.0  \n",
       "7            72000.0  2.0  \n",
       "1            66500.5  3.0  \n",
       "6            84000.5  1.0  \n",
       "2            72000.0  2.0  \n",
       "0            71000.0  1.0  \n",
       "8            66500.5  2.0  \n",
       "5            95000.6  1.0  \n",
       "3            71000.0  2.0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#14). Row Number, Rank\n",
    "# SQL--> ##SELECT A.*, ROW_NUMBER() OVER(PARTITION BY ADDRESS ORDER BY SALARY DESC)AS RN FROM DF A\n",
    "# dfpd[\"RN\"] = dfpd.sort_values(by=[\"Salary\"], ascending=False).groupby(\"Address\").cumcount()+1\n",
    "# dfpd.sort_values(by=[\"Address\",\"RN\"]).head(100)\n",
    "\n",
    "dfpd[\"RN\"] = dfpd.groupby([\"Address\"])[\"Salary\"].rank(method='max',  ascending=False)\n",
    "dfpd.sort_values(by=[\"Address\",\"RN\"]).head(100)  #method=min,max,average,first,dense\n",
    "#first --> ROW_NUMBER() in SQL, #max, min--> Rank() in SQL  #dense --> Dense_Rank() in SQL\n",
    "\n",
    "\n",
    "dfps5 = dfps.withColumn(\"RN\", rank().over(Window.partitionBy(\"Address\").orderBy(col(\"Salary\").desc()))).show() \n",
    "#row_number(), rank(), dense_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+----------+-----+-------+-----------------+-----------------+\n",
      "|Name|Weight|Address|       DOB|Batch| Salary|Salary_Hike_1year|Salary_Hike_2year|\n",
      "+----+------+-------+----------+-----+-------+-----------------+-----------------+\n",
      "|   A|    70|    HYD|1990-01-15| 2016|51000.0|          61000.0|          71000.0|\n",
      "+----+------+-------+----------+-----+-------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfps3.createOrReplaceTempView(\"DFPS3\")\n",
    "abcd = spark.sql(\"SELECT *FROM DFPS3 WHERE NAME='A' \")\n",
    "abcd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
